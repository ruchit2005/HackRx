{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpWsOaxw0N8e"
      },
      "source": [
        "# Extracting the text from the doc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYjkVWPjqYoT",
        "outputId": "4d5c4594-a7fe-4afc-caa3-adb736cd2bc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.26.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "is9wmChzqveL"
      },
      "outputs": [],
      "source": [
        "import pymupdf\n",
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get(\"HUGGINGFACE_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Plx7Lf3bq9uM",
        "outputId": "5d7aa28b-2723-4d52-c3fa-64b01571da3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.26.3\n"
          ]
        }
      ],
      "source": [
        "print(pymupdf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oLmsUzsrBGd"
      },
      "outputs": [],
      "source": [
        "file = \"/content/pp1.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "lowhCMtZr_Et",
        "outputId": "a8d1f877-c2b0-4c3c-f06b-ee93e14cf8fd"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "no such file: '/content/pp1.pdf'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-3173138230.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mpymupdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# open document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mextracted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# write as a binary file to support non-ASCII characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymupdf/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[0m\n\u001b[1;32m   2955\u001b[0m                 \u001b[0;31m# generating warnings etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2956\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2957\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"no such file: '{filename}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2958\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mFileDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{filename}' is no file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: no such file: '/content/pp1.pdf'"
          ]
        }
      ],
      "source": [
        "import sys, pathlib\n",
        "with pymupdf.open(file) as doc:  # open document\n",
        "    extracted = chr(12).join([page.get_text() for page in doc])\n",
        "# write as a binary file to support non-ASCII characters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP7urU0FtH8P"
      },
      "outputs": [],
      "source": [
        "print(extracted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cUoEbI84Guu"
      },
      "outputs": [],
      "source": [
        "# Preprocessing the document\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "\n",
        "# Normalize unicode and whitespace\n",
        "extracted = unicodedata.normalize(\"NFKC\", extracted)\n",
        "extracted = extracted.replace('\\r\\n', '\\n')\n",
        "extracted = re.sub(r'[ \\t]+', ' ', extracted)\n",
        "extracted = re.sub(r'\\s+', ' ', extracted).strip()\n",
        "\n",
        "# Remove repeated lines (common for headers/footers)\n",
        "lines = extracted.split('\\n')\n",
        "line_counts = Counter(lines)\n",
        "extracted = '\\n'.join([line for line in lines if line_counts[line] < 3])\n",
        "\n",
        "# Remove page numbers, headers/footers, and boilerplate patterns\n",
        "extracted = re.sub(r'Page\\s*\\d+(\\s*of\\s*\\d+)?', '', extracted, flags=re.IGNORECASE)\n",
        "extracted = re.sub(r'\\f', '', extracted)  # form feed from PDF\n",
        "extracted = re.sub(r'^\\s*\\d+\\s*$', '', extracted, flags=re.MULTILINE)\n",
        "extracted = re.sub(r'(Company Name|Confidential|Insurance Ltd.)', '', extracted, flags=re.IGNORECASE)\n",
        "\n",
        "# Normalize bullets and remove unwanted characters\n",
        "extracted = re.sub(r'[•–—]', '-', extracted)   # Normalize bullet characters\n",
        "extracted = re.sub(r'[^\\x00-\\x7F]+', ' ', extracted)  # Remove non-ASCII characters\n",
        "\n",
        "# Remove all-uppercase lines (often section titles or noise)\n",
        "lines = extracted.split('\\n')\n",
        "extracted = '\\n'.join([line for line in lines if not line.strip().isupper()])\n",
        "\n",
        "# Fix broken line breaks in the middle of sentences\n",
        "extracted = re.sub(r'(?<!\\n)\\n(?![\\n])', ' ', extracted)\n",
        "\n",
        "# Final cleanup: collapse whitespace again\n",
        "extracted = re.sub(r'\\s+', ' ', extracted).strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muYjNDX15uEP"
      },
      "outputs": [],
      "source": [
        "extracted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78SbGB1B755p"
      },
      "source": [
        "# Dividing into chunks and setting up RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "092VVUu173VX"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Define the splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,      # Token or char size per chunk\n",
        "    chunk_overlap=100,    # Overlap for context\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# Split the cleaned `extracted` text\n",
        "chunks = text_splitter.split_text(extracted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3V-y8B_BjT6"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain-community\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FhIWfC6FPdu"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate langchain langchain-community\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QMpxW7kCJ1t"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFbd6WLw8Hei"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Load embedding model (or use OpenAIEmbeddings if using OpenAI)\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Create the FAISS vector store\n",
        "vectorstore = FAISS.from_texts(chunks, embedding_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6-s39mGKPUE"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers accelerate huggingface_hub langchain langchain-community\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXxYosWyLLdu"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=HF_TOKEN)  # Paste your real token here (read access is enough)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QA24VUcZCnq9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=\"auto\", use_auth_token=True)\n",
        "\n",
        "# Build the HF pipeline\n",
        "mistral_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.2,\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "# Wrap for LangChain\n",
        "llm = HuggingFacePipeline(pipeline=mistral_pipe)\n",
        "\n",
        "# Use your existing vectorstore and build retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# Build RAG chain\n",
        "rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHyq8zSXNl5W"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_v6OkeoOhed"
      },
      "outputs": [],
      "source": [
        "query = \"46-year-old male, knee surgery in Pune, 3-month-old insurance policy\"\n",
        "\n",
        "\n",
        "response = rag_chain.invoke(query)\n",
        "\n",
        "print(\"Answer:\", response['result'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoscMmqc1UWj"
      },
      "source": [
        "# Setting up LLM for analyzing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtFOYMRZygXp"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "from google.colab import userdata\n",
        "\n",
        "HF_TOKEN = userdata.get(\"HUGGINGFACE_API_KEY\")\n",
        "\n",
        "client = InferenceClient(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    token=HF_TOKEN,\n",
        ")\n",
        "\n",
        "\n",
        "prompt = \"46M, knee surgery, Pune, 3-month policy\"\n",
        "\n",
        "# Use chat_completion instead of chat.completions.create\n",
        "response = client.chat_completion(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"You are a legal insurance and policy manager. Based on the following policy document:\\n\\n{extracted}\\n\\nDecide if the claim is valid, covered under the policy, and cite the clause and reasoning.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    ],\n",
        "\n",
        ")\n",
        "\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmcFwc9q6crU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
